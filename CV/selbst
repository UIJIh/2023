{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587d9314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "img1 = cv2.imread('C:/Users/h.jpg')\n",
    "img1 = cv2.imread('C:/Users/h.jpg', 1)\n",
    "img1 = cv2.imread('C:/Users/h.jpg', cv2.IMREAD_COLOR)\n",
    "\n",
    "print(type(img1))\n",
    "\n",
    "cv2.imshow('img1', img1)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d197cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "img= cv2.imread('C:/Users/h.jpg')\n",
    "img1 = cv2.imread('C:/Users/h.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "img2 = cv2.imread('C:/Users/h.jpg', cv2.IMREAD_COLOR)\n",
    "img3 = cv2.imread('C:/Users/h.jpg', cv2.IMREAD_ANYDEPTH)\n",
    "img4 = cv2.imread('C:/Users/h.jpg', cv2.IMREAD_LOAD_GDAL)\n",
    "\n",
    "cv2.imshow('img', img)\n",
    "cv2.imshow('img1', img1)\n",
    "cv2.imshow('img2', img2)\n",
    "cv2.imshow('img3', img3)\n",
    "cv2.imshow('img4', img4)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef7ee3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54cf741",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('C:/Users/h.jpg')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "plt.imshow(gray, cmap='gray')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9ad738",
   "metadata": {},
   "outputs": [],
   "source": [
    "xml = 'C:\\\\Users\\haarcascade_frontalface_default.xml'\n",
    "face_cascade = cv2.CascadeClassifier(xml)\n",
    "faces = face_cascade.detectMultiScale(gray, 1.2, 5)\n",
    "\n",
    "print(\"Number of faces detected: \" + str(len(faces)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705470ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04a8bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(faces):\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(image,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), cmap='gray')\n",
    "plt.xticks([]), plt.yticks([]) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9098b41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xml = 'C:\\\\Users\\haarcascade_smile.xml'\n",
    "smile_cascade = cv2.CascadeClassifier(xml)\n",
    "smile = face_cascade.detectMultiScale(gray, 1.2, 5)\n",
    "\n",
    "print(\"Number of smile detected: \" + str(len(faces)))\n",
    "\n",
    "if len(smile):\n",
    "    for (x,y,w,h) in smile:\n",
    "        cv2.rectangle(image,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), cmap='gray')\n",
    "plt.xticks([]), plt.yticks([]) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0abb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xml = 'C:\\\\Users\\haarcascade_eye.xml'\n",
    "eye_cascade = cv2.CascadeClassifier(xml)\n",
    "eye = face_cascade.detectMultiScale(gray, 1.2, 5)\n",
    "\n",
    "print(\"Number of eyes detected: \" + str(len(faces)))\n",
    "\n",
    "if len(eye):\n",
    "    for (x,y,w,h) in eye:\n",
    "        cv2.rectangle(image,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), cmap='gray')\n",
    "plt.xticks([]), plt.yticks([]) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcfa353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "image = cv2.imread('C:/Users/gang.jpg')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "xml = 'C:\\\\Users\\haarcascade_frontalface_default.xml'\n",
    "face_cascade = cv2.CascadeClassifier(xml)\n",
    "faces = face_cascade.detectMultiScale(gray, 1.2, 5)\n",
    "\n",
    "print(\"사람 얼굴: \" + str(len(faces)))\n",
    "\n",
    "if len(faces):\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(image,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), cmap='gray')\n",
    "plt.xticks([]), plt.yticks([]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b3eed8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "image = cv2.imread('C:/Users/gang.jpg')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "xml = 'C:\\\\Users\\haarcascade_fullbody.xml'\n",
    "face_cascade = cv2.CascadeClassifier(xml)\n",
    "faces = face_cascade.detectMultiScale(gray, 1.2, 5)\n",
    "\n",
    "print(\"과연 강승민의 몸뚱아리는 :  \" + str(len(faces)) + \"개\")\n",
    "\n",
    "if len(faces):\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(image,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), cmap='gray')\n",
    "plt.xticks([]), plt.yticks([]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80183cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "# Cascades 디렉토리의 haarcascade_frontalface_default.xml 파일을 Classifier로 사용\n",
    "# faceCascade는 이미 학습 시켜놓은 XML 포멧이고, 이를 불러와서 변수에 저장함.\n",
    "faceCascade = cv2.CascadeClassifier('C:/Users/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# 비디오의 setting을 준비함.\n",
    "cap = cv2.VideoCapture(0) #0번이 내장카메라, 1번이 외장카메라\n",
    "cap.set(3,1280) # set Width\n",
    "cap.set(4,720) # set Height\n",
    "\n",
    "\n",
    "while True:\n",
    "    # video의 이미지를 읽어옴\n",
    "    ret, img = cap.read()\n",
    "    #img = cv2.flip(img, 1) # 상하반전\n",
    "    # 이후 얼굴을 검출할 gray scale을 만듦\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) #make grayscale\n",
    "    faces = faceCascade.detectMultiScale( #이미지에서 얼굴을 검출\n",
    "        gray, #grayscale로 이미지 변환한 원본.\n",
    "        scaleFactor=1.2, #이미지 피라미드에 사용하는 scalefactor\n",
    "        #scale 안에 들어가는 이미지의 크기가 1.2씩 증가 즉 scale size는 그대로\n",
    "        # 이므로 이미지가 1/1.2 씩 줄어서 scale에 맞춰지는 것이다.\n",
    "        minNeighbors=3, #최소 가질 수 있는 이웃으로 3~6사이의 값을 넣어야 detect가 더 잘된다고 한다.\n",
    "        #Neighbor이 너무 크면 알맞게 detect한 rectangular도 지워버릴 수 있으며,\n",
    "        #너무 작으면 얼굴이 아닌 여러개의 rectangular가 생길 수 있다.\n",
    "        #만약 이 값이 0이면, scale이 움직일 때마다 얼굴을 검출해 내는 rectangular가 한 얼굴에\n",
    "        #중복적으로 발생할 수 있게 된다.\n",
    "        minSize=(20, 20) #검출하려는 이미지의 최소 사이즈로 이 크기보다 작은 object는 무시\n",
    "        #maxSize도 당연히 있음.\n",
    "    )\n",
    "    for (x,y,w,h) in faces: #좌표 값과 rectangular의 width height를 받게 된다.\n",
    "        #x,y값은 rectangular가 시작하는 지점의 좌표\n",
    "        #원본 이미지에 얼굴의 위치를 표시하는 작업을 함.\n",
    "        #for문을 돌리는 이유는 여러 개가 검출 될 수 있기 때문.\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,255,255),2)\n",
    "        #다른 부분, 얼굴 안에 들어있는 눈과 입 등을 검출할 때 얼굴 안엣 검출하라는 의미로 이용되는 것\n",
    "        roi_gray = gray[y:y+h, x:x+w] #눈,입을 검출할 때 이용\n",
    "        roi_color = img[y:y+h, x:x+w] #눈,입등을 표시할 때 이용\n",
    "    #영상에 img 값을 출력\n",
    "    cv2.imshow('video',img) # video라는 이름으로 출력\n",
    "    k = cv2.waitKey(1) & 0xff #time값이 0이면 무한 대기, waitKey는 키가 입력 받아 질때까지 기다리는 시간을 의미한다.\n",
    "    #FF는 끝의 8bit만을 이용한다는 뜻으로 ASCII 코드의 0~255값만 이용하겠다는 의미로 해석됨. (NumLock을 켰을때 또한 )\n",
    "    if k == 27: # press 'ESC' to quit # ESC를 누르면 종료\n",
    "        break\n",
    "cap.release() #비디오 끄기   (카메라 리소스 헤제)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae84f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "path = \"C:/Users/gang.jpg\" # 사진 파일의 디렉토리\n",
    "display(Image(filename = path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5f36f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import cvlib as cv # cvlib 미설치 시 !pip install cvlib으로 설치 진행\n",
    "\n",
    "img = cv2.imread(path) # 이미지 파일 불러오기\n",
    "conf = 0.5 # 사물 인식을 진행할 confidence의 역치 값\n",
    "model_name = \"yolov3\" # 사물을 인식할 모델 이름\n",
    "\n",
    "result = cv.detect_common_objects(img, confidence=conf, model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51d5960",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"/cat_dog_detect.jpg\" # 결과가 반영된 이미지 파일 저장 디렉토리\n",
    "\n",
    "result_img = cv.object_detection.draw_bbox(img, *result) # result 결과를 이미지에 반영\n",
    "cv2.imwrite(output_path, result_img) # 반영된 이미지 파일 저장\n",
    "display(Image(filename = output_path)) # 이미지 출력"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
